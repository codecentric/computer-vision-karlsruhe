{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV by Example\n",
    "\n",
    "In diesem Notebook werden euch einige Funktionen der Bibliothek OpenCV vorgestellt. OpenCV ist eine populäre Software Biblothek für Computer Vision. Die Funktionalitäten von OpenCV reichen von Bildmanipulation bis hin zur Klassifikation von verschiedenen Bildregionen zu Objekten. OpenCV stellt standardmäßig Interfaces für C++, Python und Java bereit. In diesem Notebook wird Python verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import von OpenCV\n",
    "Um OpenCV zu verwenden muss die Bibliothek zuerst importiert werden. In Python werden Bibliotheken mit dem **import** Statement implementiert. OpenCV wird in Python als **cv2** Modul in der Regel bezeichnet. Weiterhin benötigen wir noch das Matplotlib um die Bilder im Jupyter Notebook anzuzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Magic Function: Dadurch werden die Plots/Bilder im Notebook dargestellt.\n",
    "%matplotlib inline \n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen von Bilder\n",
    "Bilder werden mit der Funktion **cv2.imread(pfad)** eingelesen. Dabei können die Bilder in unterschiedlichen Formaten wie jpg oder png und noch viele weiteren vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_image = \"../resources/home.jpg\"\n",
    "image = cv2.imread(path_to_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interne Darstellung der Bilder\n",
    "Bilder werden als Matrix dargestellt. Mit der Methode **image.shape** erhalten wir die Dimensionen des Bilds. Das eingelesene Bild hat die Werte *(384, 512, 3)*. Das bedeutet, dass das Bild eine Höhe von 384, eine Breite von 512 und 3 Kanäle für einen Pixel hat. Die bekannteste Darstellung der 3-Kanäle ist die Rot-Grün-Blau-Darstellung (RGB). Bei OpenCV werden die Bilder standardmäßig als Blau-Grün-Rot-Darstellung (BGR) verarbeitet. \n",
    "\n",
    "Format von image.shape: **(Höhe, Breite, Kanäle)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgeben der Bilder\n",
    "Zum Ausgeben der Bilder verwenden wir die Bibliothek Matplotlib, da wir in einem jupyter notebook arbeiten und dadurch nicht die Ausgabefunktion von OpenCV verwenden können. OpenCV verarbeitet die Bilder intern standardmäßig in einem BGR Kanal. Das erste Bild zeigt das Bild im BGR Kanal. Beim zweiten Bild wandeln wir BGR zu RGB mittels der Funktion *cv2.cvtColor(bild, kanal)* um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl der Werte eines Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# lese Farbwerte an Position y, x\n",
    "y = 100\n",
    "x = 50\n",
    "\n",
    "# erhalten der Werte der Kanäle \n",
    "(r, g, b) = rgb_image[y, x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswählen aller Werte der einzelnen Kanäle\n",
    "Um die Werte eines einzelnen Kanal auszuwählen, müssen wir zunächst nochmal einen Blick auf die Darstellung der Werte stellen. Dazu orientieren wir uns am **shape** Attribut des Bilds, dass im vorherigen Abschnitt vorgestellt wurde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_channel = rgb_image[:,:, 0]\n",
    "green_channel = rgb_image[:,:, 1]\n",
    "blue_channel = rgb_image[:,:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# lese Farbwerte an Position y, x\n",
    "y = 100\n",
    "x = 50\n",
    "\n",
    "# erhalten der Werte der Kanäle \n",
    "(r, g, b) = rgb_image[y, x]\n",
    " \n",
    "# gib Farbwerte auf Bildschirm aus\n",
    "print(r,g,b)\n",
    " \n",
    "# setze Farbwerte auf Rot (im RGB-Farbraum)\n",
    "rgb_image[y, x] = (255, 0, 0)\n",
    "\n",
    "# ausgeben der Werte des manipulierten Bilds\n",
    "print(rgb_image[y,x])\n",
    "plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waehle eine Region of Interest (ROI) an Punkt: (y, x) mit Dimension 50x50 Pixel\n",
    "region_of_interest = rgb_image[y:y+50, x:x+50]\n",
    "\n",
    "# setze ROI auf Gruen\n",
    "region_of_interest[:, :] = (255, 0, 0)\n",
    "plt.imshow(region_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "Mit Thresholding können mit Schwellwerten verschiedene Pixelbereiche in einem Bild selektiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "selected_pixels = cv2.inRange(rgb_image, (0, 50, 50), (255,150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(selected_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objekte zeichnen\n",
    "Mit OpenCV können standardmäßig geometrische Objekte wie Linien, Rechtecke oder Kreise in ein Bild gezeichnet werden. Bevor diese Funktionen auf das Bild anwenden, kopieren wir davor das RGB Bild mit **copy()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lines\n",
    "# Kopieren des Bilds\n",
    "line_img = rgb_image.copy()\n",
    "cv2.line(line_img, (50, 50), (400, 350), (255, 0, 0), thickness=5)\n",
    "plt.imshow(line_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rectangle\n",
    "rect_img = rgb_image.copy()\n",
    "cv2.rectangle(rect_img, (50, 50), (400, 350), (255, 0, 0), thickness=-1 )\n",
    "plt.imshow(rect_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Circle\n",
    "circle_img = rgb_image.copy()\n",
    "height, width = rgb_image.shape[:2] \n",
    "cv2.circle(circle_img, (int(width/2)-5, int(height/2)-5), 10, (255, 0, 0), thickness=-1)\n",
    "plt.imshow(circle_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haar Cascade Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haar Cascade Classifier ist eine Methode um Objekterkennung in Bilder vorzunehmen. Dazu werden aus dem Bild Features anhand von Convolutional Kernels entnommen. Ein Kernel ist eine Art Filter die sich über verschiedene Regionen des Bildes schiebt und dabei Werte extrahiert und eine mathematische Operation ausführt. Bei den Haar Cascade Classifer sehen die Kernel wie in diesem [Beispielbild](https://docs.opencv.org/trunk/haar_features.jpg) aus. Dabei werden die Pixel in der weißen und schwarzen Fläche jeweils aufsummiert und anschließend voneinander abgezogen. Diese Werte entsprechen dann den Features.\n",
    "\n",
    "\n",
    "Dadurch entsteht eine große Anzahl an Features von denen die meisten irrelevant für die Klassifikation sind. Mithilfe von Adaboost werden die wichtigsten Features selektiert. Dazu wird jedes der Features auf die Trainingsbilder angewandt. Anschließend werden die Trainingsbilder klassifiziert. Ein Beispiel für eine Klassifikation ist die Gesichtserkennung, bei der ein Klassifizier bestimmt, ob auf dem Bild ein Gesicht zu sehen ist oder nicht (0/1). Da dabei viele Missklassifikationen entstehen werden nur die Features ausgewählt, die die geringste Fehlerrate bei der Klassifikation haben. \n",
    "\n",
    "Der finale Klassifizierer besteht dann aus den besimmten Features. Für die Klassifikation bewegt sich ein Sliding Window durch das Bild. Das Sliding Window repräsentiert einen kleien Teilbereich des Bilds und extrahiert die Bilder. Am besten versteht man das Konzept, wenn man sich diese [Beispiel](https://www.pyimagesearch.com/wp-content/uploads/2015/03/sliding-window-animated-sot.gif) anschaut. Da hier pro Region alle Features extrahiert werden, ist der Algorithmus aus Sicht der Performance kritisch. Um das Verfahren zu beschleunigen durchläuft eine Region mehrere Stages. In einer Stage wird eine Teil der Features extrahiert und klassifiziert. Wenn bei der Klassifizierung \"etwas gefunden\" wurde, wird die nächste Stage auf die Region angewandt, andernfalls abgebrochen. Aus diesem Grund heißt diese Konzept **Cascade of Classifiers**.\n",
    "\n",
    "Quelle: https://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html\n",
    "\n",
    "Standardmäßig enthält OpenCV vortrainierte Model die im Installationsverzeichnis von OpenCV im Pfad *data/haarcascades/* abgespeichert sind. Wenn ihr euch nicht auf der AWS Instanz befindet, müsst ihr den folgenden Pfad ändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opencv_data_haarcascade = '/home/johndoedude/opencv/data/haarcascades'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(opencv_data_haarcascade + '/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../resources/img_649.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# Das Bild wird zur Graustufendarstellung umgewandelt.\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eye Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier(opencv_data_haarcascade + '/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Wir haben hier ein Auge mehrfach erkannt. In der Computer Vision gibt es dafür Algorithmen um solche Sachen zu präventieren bzw. algorithmisch zu verhindern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian detector\n",
    "Neben dem Haar Cascade Classifier verfügt OpenCV über Histogram of Oriented Graidents (HOG) und Linear Support Vector Machine (SVM) um Objekte zu erkennen. HOG ist eine Methode um Merkmale aus einem Bild zu extrahieren die anschließend mit SVM klassifiziert werden. Falls euch die Details von HOG interessieren, empfehlen wir euch diese Blog Artikel:[Histogram of Oriented Gradients](https://www.learnopencv.com/histogram-of-oriented-gradients/) und [Histogram of Oriented Gradients and Object Detection](https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../resources/FudanPed00010.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect people in the image\n",
    "(rects, weights) = hog.detectMultiScale(image, winStride=(4, 4),\n",
    "    padding=(8, 8), scale=1.5)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
